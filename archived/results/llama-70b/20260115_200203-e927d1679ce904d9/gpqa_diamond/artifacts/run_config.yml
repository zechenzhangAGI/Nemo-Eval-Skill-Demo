command: '{% if target.api_endpoint.api_key is not none %}export API_KEY=${{target.api_endpoint.api_key}}
  && {% endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
  is not none %} python3 -c ''import yaml, json, sys; config_data = {{config.params.extra.custom_config
  | tojson}}; json.dump(config_data, open("{{config.output_dir}}/temp_config.json",
  "w")); yaml.dump(config_data, open("{{config.output_dir}}/custom_config.yml", "w"),
  default_flow_style=False)'' && {% endif %} simple_evals --model {{target.api_endpoint.model_id}}
  --eval_name {{config.params.task}} --url {{target.api_endpoint.url}} --temperature
  {{config.params.temperature}} --top_p {{config.params.top_p}} --max_tokens {{config.params.max_new_tokens}}
  --out_dir {{config.output_dir}}/{{config.type}} --cache_dir {{config.output_dir}}/{{config.type}}/cache
  --num_threads {{config.params.parallelism}} --max_retries {{config.params.max_retries}}
  --timeout {{config.params.request_timeout}} {% if config.params.extra.n_samples
  is defined %} --num_repeats {{config.params.extra.n_samples}}{% endif %} {% if config.params.limit_samples
  is not none %} --first_n {{config.params.limit_samples}}{% endif %} {% if config.params.extra.add_system_prompt  %}
  --add_system_prompt {% endif %} {% if config.params.extra.downsampling_ratio is
  not none %} --downsampling_ratio {{config.params.extra.downsampling_ratio}}{% endif
  %} {% if config.params.extra.args is defined %} {{ config.params.extra.args }} {%
  endif %} {% if config.params.extra.judge.url is not none %} --judge_url {{config.params.extra.judge.url}}{%
  endif %} {% if config.params.extra.judge.model_id is not none %} --judge_model_id
  {{config.params.extra.judge.model_id}}{% endif %} {% if config.params.extra.judge.api_key
  is not none %} --judge_api_key_name {{config.params.extra.judge.api_key}}{% endif
  %} {% if config.params.extra.judge.backend is not none %} --judge_backend {{config.params.extra.judge.backend}}{%
  endif %} {% if config.params.extra.judge.request_timeout is not none %} --judge_request_timeout
  {{config.params.extra.judge.request_timeout}}{% endif %} {% if config.params.extra.judge.max_retries
  is not none %} --judge_max_retries {{config.params.extra.judge.max_retries}}{% endif
  %} {% if config.params.extra.judge.temperature is not none %} --judge_temperature
  {{config.params.extra.judge.temperature}}{% endif %} {% if config.params.extra.judge.top_p
  is not none %} --judge_top_p {{config.params.extra.judge.top_p}}{% endif %} {% if
  config.params.extra.judge.max_tokens is not none %} --judge_max_tokens {{config.params.extra.judge.max_tokens}}{%
  endif %} {% if config.params.extra.judge.max_concurrent_requests is not none %}
  --judge_max_concurrent_requests {{config.params.extra.judge.max_concurrent_requests}}{%
  endif %} {% if config.params.extra.custom_config is defined and config.params.extra.custom_config
  is not none %} --custom_eval_cfg_file {{config.output_dir}}/custom_config.yml{%
  endif %}'
config:
  output_dir: /results
  params:
    extra:
      add_system_prompt: false
      custom_config: null
      downsampling_ratio: null
      judge:
        api_key: null
        backend: openai
        max_concurrent_requests: null
        max_retries: 16
        max_tokens: 1024
        model_id: null
        request_timeout: 600
        temperature: 0.0
        top_p: 0.0001
        url: null
    limit_samples: null
    max_new_tokens: 2048
    max_retries: 5
    parallelism: 4
    request_timeout: 60
    task: gpqa_diamond
    temperature: 0.6
    top_p: 1.0e-05
  supported_endpoint_types:
  - chat
  type: gpqa_diamond
framework_name: simple_evals
pkg_name: simple_evals
target:
  api_endpoint:
    adapter_config:
      discovery:
        dirs: []
        modules: []
      endpoint_type: chat
      interceptors:
      - config:
          cache_dir: /results/cache
          max_saved_requests: 5
          max_saved_responses: 5
          reuse_cached_responses: true
          save_requests: true
          save_responses: true
        enabled: true
        name: caching
      - config: {}
        enabled: true
        name: endpoint
      - config:
          cache_dir: /results/response_stats_cache
          logging_aggregated_stats_interval: 100
        enabled: true
        name: response_stats
      log_failed_requests: false
      post_eval_hooks:
      - config:
          html_report_size: 5
          report_types:
          - html
          - json
        enabled: true
        name: post_eval_report
    api_key: API_KEY
    model_id: meta/llama-3.1-70b-instruct
    stream: null
    type: chat
    url: https://integrate.api.nvidia.com/v1/chat/completions
