command: 'export API_KEY=$API_KEY &&   simple_evals --model meta/llama-3.1-405b-instruct
  --eval_name gpqa_diamond --url https://integrate.api.nvidia.com/v1/chat/completions
  --temperature 0.6 --top_p 1e-05 --max_tokens 2048 --out_dir /results/gpqa_diamond
  --cache_dir /results/gpqa_diamond/cache --num_threads 3 --max_retries 5 --timeout
  60          --judge_backend openai  --judge_request_timeout 600  --judge_max_retries
  16  --judge_temperature 0.0  --judge_top_p 0.0001  --judge_max_tokens 1024  '
config:
  output_dir: /results
  params:
    extra: {}
    limit_samples: null
    max_new_tokens: 2048
    max_retries: null
    parallelism: 3
    request_timeout: null
    task: null
    temperature: 0.6
    top_p: null
  supported_endpoint_types: null
  type: gpqa_diamond
git_hash: ''
metadata:
  __skipped_fields: see metadata.yaml for the rest of the fields
  versioning:
    nemo_evaluator: 0.1.39
    nemo_evaluator_launcher: 0.1.67
results:
  groups:
    gpqa_diamond:
      metrics:
        score:
          scores:
            micro:
              stats:
                stddev: 0.4999744917480639
                stderr: 0.03562170760625403
              value: 0.5050505050505051
  tasks:
    gpqa_diamond:
      metrics:
        score:
          scores:
            micro:
              stats:
                stddev: 0.4999744917480639
                stderr: 0.03562170760625403
              value: 0.5050505050505051
target:
  api_endpoint:
    adapter_config:
      discovery:
        dirs: []
        modules: []
      endpoint_type: chat
      interceptors:
      - config:
          cache_dir: /results/cache
          max_saved_requests: 5
          max_saved_responses: 5
          reuse_cached_responses: true
          save_requests: true
          save_responses: true
        enabled: true
        name: caching
      - config: {}
        enabled: true
        name: endpoint
      - config:
          cache_dir: /results/response_stats_cache
          logging_aggregated_stats_interval: 100
        enabled: true
        name: response_stats
      log_failed_requests: false
      post_eval_hooks:
      - config:
          html_report_size: 5
          report_types:
          - html
          - json
        enabled: true
        name: post_eval_report
    api_key: API_KEY
    model_id: meta/llama-3.1-405b-instruct
    stream: null
    type: chat
    url: https://integrate.api.nvidia.com/v1/chat/completions
